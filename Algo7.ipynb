{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgcVfRatJ2ghsEqnfc2ta+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# objective function\n","def objective(x, y):\n","\treturn x**2.0 + y**2.0\n","\n","# 3d plot of the test function\n","from numpy import arange\n","from numpy import meshgrid\n","from matplotlib import pyplot\n","\n","# objective function\n","def objective(x, y):\n","\treturn x**2.0 + y**2.0\n","\n","# define range for input\n","r_min, r_max = -1.0, 1.0\n","# sample input range uniformly at 0.1 increments\n","xaxis = arange(r_min, r_max, 0.1)\n","yaxis = arange(r_min, r_max, 0.1)\n","# create a mesh from the axis\n","x, y = meshgrid(xaxis, yaxis)\n","# compute targets\n","results = objective(x, y)\n","# create a surface plot with the jet color scheme\n","figure = pyplot.figure()\n","axis = figure.gca(projection='3d')\n","axis.plot_surface(x, y, results, cmap='jet')\n","# show the plot\n","pyplot.show()\n","\n","# contour plot of the test function\n","from numpy import asarray\n","from numpy import arange\n","from numpy import meshgrid\n","from matplotlib import pyplot\n","\n","# objective function\n","def objective(x, y):\n","\treturn x**2.0 + y**2.0\n","\n","# define range for input\n","bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n","# sample input range uniformly at 0.1 increments\n","xaxis = arange(bounds[0,0], bounds[0,1], 0.1)\n","yaxis = arange(bounds[1,0], bounds[1,1], 0.1)\n","# create a mesh from the axis\n","x, y = meshgrid(xaxis, yaxis)\n","# compute targets\n","results = objective(x, y)\n","# create a filled contour plot with 50 levels and jet color scheme\n","pyplot.contourf(x, y, results, levels=50, cmap='jet')\n","# show the plot\n","pyplot.show()\n","\n","# derivative of objective function\n","def derivative(x, y):\n","\treturn asarray([x * 2.0, y * 2.0])\n","# generate an initial point\n","solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n","\n","...\n","# list of the sum square gradients for each variable\n","sq_grad_sums = [0.0 for _ in range(bounds.shape[0])]\n","\n","...\n","# run the gradient descent\n","for it in range(n_iter):\n","\t...\n","\n","...\n","# calculate gradient\n","gradient = derivative(solution[0], solution[1])\n","\n","...\n","# update the sum of the squared partial derivatives\n","for i in range(gradient.shape[0]):\n","\tsq_grad_sums[i] += gradient[i]**2.0\n","\n","\t...\n","# build a solution one variable at a time\n","new_solution = list()\n","for i in range(solution.shape[0]):\n","\t# calculate the step size for this variable\n","\talpha = step_size / (1e-8 + sqrt(sq_grad_sums[i]))\n","\t# calculate the new position in this variable\n","\tvalue = solution[i] - alpha * gradient[i]\n","\t# store this variable\n","\tnew_solution.append(value)\n","\n","\t...\n","# evaluate candidate point\n","solution = asarray(new_solution)\n","solution_eval = objective(solution[0], solution[1])\n","# report progress\n","print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n","\n","# gradient descent algorithm with adagrad\n","def adagrad(objective, derivative, bounds, n_iter, step_size):\n","\t# generate an initial point\n","\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n","\t# list of the sum square gradients for each variable\n","\tsq_grad_sums = [0.0 for _ in range(bounds.shape[0])]\n","\t# run the gradient descent\n","\tfor it in range(n_iter):\n","\t\t# calculate gradient\n","\t\tgradient = derivative(solution[0], solution[1])\n","\t\t# update the sum of the squared partial derivatives\n","\t\tfor i in range(gradient.shape[0]):\n","\t\t\tsq_grad_sums[i] += gradient[i]**2.0\n","\t\t# build a solution one variable at a time\n","\t\tnew_solution = list()\n","\t\tfor i in range(solution.shape[0]):\n","\t\t\t# calculate the step size for this variable\n","\t\t\talpha = step_size / (1e-8 + sqrt(sq_grad_sums[i]))\n","\t\t\t# calculate the new position in this variable\n","\t\t\tvalue = solution[i] - alpha * gradient[i]\n","\t\t\t# store this variable\n","\t\t\tnew_solution.append(value)\n","\t\t# evaluate candidate point\n","\t\tsolution = asarray(new_solution)\n","\t\tsolution_eval = objective(solution[0], solution[1])\n","\t\t# report progress\n","\t\tprint('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n","\treturn [solution, solution_eval]\n","\n","# seed the pseudo random number generator\n","seed(1)\n","# define range for input\n","bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n","# define the total iterations\n","n_iter = 50\n","# define the step size\n","step_size = 0.1\n","# perform the gradient descent search with adagrad\n","best, score = adagrad(objective, derivative, bounds, n_iter, step_size)\n","print('Done!')\n","print('f(%s) = %f' % (best, score))\n","\n","# gradient descent optimization with adagrad for a two-dimensional test function\n","from math import sqrt\n","from numpy import asarray\n","from numpy.random import rand\n","from numpy.random import seed\n","\n","# objective function\n","def objective(x, y):\n","\treturn x**2.0 + y**2.0\n","\n","# derivative of objective function\n","def derivative(x, y):\n","\treturn asarray([x * 2.0, y * 2.0])\n","\n","# gradient descent algorithm with adagrad\n","def adagrad(objective, derivative, bounds, n_iter, step_size):\n","\t# generate an initial point\n","\tsolution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n","\t# list of the sum square gradients for each variable\n","\tsq_grad_sums = [0.0 for _ in range(bounds.shape[0])]\n","\t# run the gradient descent\n","\tfor it in range(n_iter):\n","\t\t# calculate gradient\n","\t\tgradient = derivative(solution[0], solution[1])\n","\t\t# update the sum of the squared partial derivatives\n","\t\tfor i in range(gradient.shape[0]):\n","\t\t\tsq_grad_sums[i] += gradient[i]**2.0\n","\t\t# build a solution one variable at a time\n","\t\tnew_solution = list()\n","\t\tfor i in range(solution.shape[0]):\n","\t\t\t# calculate the step size for this variable\n","\t\t\talpha = step_size / (1e-8 + sqrt(sq_grad_sums[i]))\n","\t\t\t# calculate the new position in this variable\n","\t\t\tvalue = solution[i] - alpha * gradient[i]\n","\t\t\t# store this variable\n","\t\t\tnew_solution.append(value)\n","\t\t# evaluate candidate point\n","\t\tsolution = asarray(new_solution)\n","\t\tsolution_eval = objective(solution[0], solution[1])\n","\t\t# report progress\n","\t\tprint('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n","\treturn [solution, solution_eval]\n","\n","# seed the pseudo random number generator\n","seed(1)\n","# define range for input\n","bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n","# define the total iterations\n","n_iter = 50\n","# define the step size\n","step_size = 0.1\n","# perform the gradient descent search with adagrad\n","best, score = adagrad(objective, derivative, bounds, n_iter, step_size)\n","print('Done!')\n","print('f(%s) = %f' % (best, score))\n"],"metadata":{"id":"7uUxSslwho5S"},"execution_count":null,"outputs":[]}]}